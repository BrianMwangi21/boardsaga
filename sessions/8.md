### Session 008 - Phase 3 Debugging & Model Testing

**Date**: December 29, 2025

**Summary**:
- Debugged Phase 3 story generation issues with AI responses
- Tested API endpoints directly via curl to identify truncation problems
- Switched from `openai/gpt-oss-120b:free` to `meta-llama/llama-3.3-70b-instruct:free` model
- Added `maxOutputTokens: 4096` to prevent response truncation
- Improved JSON parsing to handle markdown code blocks (```json ... ```)
- Added retry logic (3 attempts) for both analyze and story endpoints
- Enhanced error logging with response length and preview of truncated content
- Added test story viewer button for UI debugging

**Completed**:
- Fixed JSON parsing in both API routes:
  - analyze-game: Added support for markdown code blocks
  - generate-story: Already had proper parsing in place
- Added retry logic to analyze-game endpoint with 1s delay
- Added better error messages showing response length and last 300 chars
- Switched model from `openai/gpt-oss-120b:free` to `meta-llama/llama-3.3-70b-instruct:free` for testing
- Added `maxOutputTokens: 4096` parameter to streamText() calls in both endpoints
- Added "Test Story Viewer" button on upload page for debugging UI
- Tested full API workflow via curl:
  - parse-pgn: Working correctly
  - analyze-game: Working with truncated responses on old model
  - generate-story: Working correctly with new model and maxOutputTokens

**Files Created/Modified**:
- `app/api/analyze-game/route.ts` - Added retry logic, fixed JSON parsing, added maxOutputTokens, changed model
- `app/api/generate-story/route.ts` - Added maxOutputTokens to streamText() call
- `app/page.tsx` - Added testStoryViewer() function and "Test Story Viewer" button

**Issues Found**:
1. **Free model truncation**: `openai/gpt-oss-120b:free` was cutting off responses mid-JSON
   - Root cause: No `max_tokens` specified, using model's default (too small for full responses)
   - Fix: Switched to `meta-llama/llama-3.3-70b-instruct:free` and added `maxOutputTokens: 4096`

2. **JSON parsing failures**: LLM responses were in markdown code blocks but regex didn't handle them
   - Fix: Added `.*?```/` regex pattern to match markdown code blocks first, then fall back to plain JSON

3. **UI blank pages**: Story wasn't rendering due to missing/null data
   - Fix: Added error handling for missing chapters, fallback chapter generation, null checks in StoryViewer

**Test Results**:
- parse-pgn: ✅ Working (tested via curl)
- analyze-game: ✅ Working (tested via curl with new model)
- generate-story: ✅ Working (tested via curl, returned complete story with 3 chapters)
- Test Story Viewer: ✅ Working (UI displays simple test story correctly)

**Next Steps**:
- Test full PGN upload → analysis → story generation workflow with new model
- If successful, proceed to Phase 4: Storage & Persistence
- If issues persist, consider alternative free models on OpenRouter

**Notes**:
- The `maxOutputTokens` parameter is specific to Vercel AI SDK and OpenRouter provider
- Changed model to `meta-llama/llama-3.3-70b-instruct:free` for testing, but can be changed back if it performs poorly
- All endpoints now have retry logic (3 attempts) with 1s delay between retries
- Error messages now show response length to help debug truncation issues
